PCA constructs orthogonal - mutually uncorrelated - 

linear combinations that (successively) explains as much common variation as possible. 
Actually, PCA can be done based on the covariance matrix as well as the correlation matrix, 
not only the latter one. Scaling the data matrix such that all variables have zero mean and unit variance 
(also known as "normalizing", "studentisizing", "z-scoring"), makes the two approaches identical. 
This is because the covariance between two normalized variables *is* the correlation coefficient.
Should you log-transform?
If you have variables that always get positive numbers, such as lenght, weight, etc., and that showes much more variation with higher values (heteroscedasticity), 
a log-normal distribution (i.e., normal after log-transformation) might be a clearly better description of the data than a normal distribution. 
In such cases I would log-transform before doing PCA. 
Log-transforming that kind of variables makes the distributions more normally distributed, stabilizes the variances, but also makes your model multiplative on the raw scale instead of additive. 
That is of course the case for all types of linear models, such as t-tests or multiple regression. 
That's worth a thought when you interpret the results.
Should you scale the data to mean = 0, var = 1?
This depends on your study question and your data. As a rule of thumb, if all your variables are measured on the same scale and have the same unit, 
it might be a good idea *not* to scale the variables (i.e., PCA based on the covariance matrix). 
If you want to maximize variation, it is fair to let variables with more variation contribute more. 
On the other hand, if you have different types of variables with different units, 
it is probably wise to scale the data first (i.e., PCA based on the correlation matrix).