참고자료_1 : https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac4
참고자료_2 : https://datascience.stackexchange.com/questions/38920/does-batch-normalization-mean-that-sigmoids-work-better-than-relus

how continuous functions may seem appealing but relu is better than all of them and 
this statement is not from my side MR. Hinton quoted it "we were dumb people who were using sigmoid as an activation function and 
it took 30 years for that realization to occur that without understanding its form its's never gonna let your neuron go in learning state its always saturating so  
it's derivative and he called himself and all others dumbfounded people".

So choosing an activation function merely because it's continuous and not looking at how its gonna affect your neuron's firing potential like when you use sigmoid 
your network is bound to face the vanishing gradient problem because your activation function is itself "saturating" and you have to find the right balance between saturation(like sigmoid) 
and too much activation(don't use exponential) and that is where a linear activation like Relu finds the middle path in between and batchnorm is there to help you more.

Note: If you are studying neural nets I would advise you to think of neural nets as big and deep composite functions 
so to understand what works and why it works you need to understand how a neural net creates a manifold of data in some higher dimension "representing" that data in which the goodness of manifold depends on your choice of functions 
and how a function transforms the other functions output when given to it as input.


기본적으로 Batch normalization 은 학습 Model과 관련하여, 학습의 깊이 결정 관련성이 깊다.
기본적으로 각각의 '뉴런'과 '뉴련'사이를 이어가는 과정에서 Sigmoid Function을 사용하여, 특정 범위로 Squeeze한다고 하더라도, 이는 동기화를 위한 목적은 아니며,

특정 값들이 가지고 있는 중요도와 관련하여, Calculate 하고자 하는 값을 누실하지 않기 위해, Batch normalization과 뿐만아니라 Relu처럼 ReLU = x + max(0,x) 와 같은 Activation Function은
특정한 구조화 (Make learning structure)와 관련하여 중요하다.
