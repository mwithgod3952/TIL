자연어 처리 Basic Summary_ Draft Ver

- 산업 트렌드 정리
        Closed domain chatbot
          Keyword와  Intent 2개의 학습 결과에 기반하여 특정 경과를 도출 or 분류
              Keyword와  Intent의 개요              
              Keyword : 사용자가 던진 질문에서의 핵심 키워드를 인지 >> Named Entity Recognition(NER)
              Intent : 사용자가 어떤 종류의 질문을 하고 있는지를 분류 >> Intent Classification

- 작업 수행에 대한 괄목적인 단계
1st. Word Embedding : 단어를 벡터화 
2nd. CNN : Intent Classification >> NER
3rd. LSTM : 개체명 추출 : 

- Text Classification
분류는 크게 '이진분류'(Binary Classification)과 '다중 클래스 분류'(Multiclass Classification)으로 나눌 수 있다.
이는, 클래스 수에 의해 분류되는 구분이며, 
이진분류라면 스칼라 값 또는 2차원의 백터값을 반환하고,
다중 클래스 분류라면 클래스 개수인 N차원 백터를 반환한다.

위의 요건에 따라, 크게 Sigmoid Function과 Softmax Function 두 개의 함수를 활용하는데 특이사항은 아래와 같다. 
Sigmoid : 이진분류를 위함이며,  결과는 1개의 스칼라 값을 도출한다.
Softmax : 다중클레스 분류를 목적으로 하며, 3게 이상 중의 in에 대해, 분류하고자하는 class 갯수만큼 의 Out을 이루고
값은 힙이 100% 즉 1이 되어야 되도록 반환한다.

          ++A
            + 이진 분류를 목적으로 하더라도, Softmax Function 활용이 가능하다 단, 기존의 Sigmoid Function과 다른 점은
            이진분류를 원하더라도 Sigmoid가 1개의 스칼라 값을 반환하는 반면에, Softmax function은 2개의 Out을 통해 이진분류가 가능해진다.
 
 Neural Network 
 Basics of Neural Network
 가장 기본적인 Neural Network Model로 ANN(Artificial neural network)를 예로 들 수 있는데, 이는 
 In * Out 갯수 만큼의 W(eight)를 필요로하며, + 편향 즉, In * Out + Oot 만큼의 Weight값을 필요로한다.
 
예를 들어, 3개의 In과 2Neuran을 가진 1개의 Hidden layer 및 3개의 Out을 목표로 한다고 가정해보자,
이 ANN 을 위해 학습되는 Weight의 개수는, (3 * 2 + 2) + (2 * 3 + 3) = 17개의 Weight가 필요함을 예상할 수 있다.
 
        + 통상 Hidden layer에는 Relu activation function을, Out에는 Soft max activation function을 사용하낟.
 
 RNN과 CNN
 1. RNN(Recurrent Neural Network)
 : 연속적인 시퀀스를 처리하기 위한 신경망이다. 사람이 글을 읽을 때, 이전 단어들에 대한 이해를 바탕으로 다음 단어를 이해하는 구조와 같이,
 RNN은 이러한 이슈를 다루며, 내부에 정보를 지속하는 루프로 구성된 신경망이라 볼 수 있다.
 
  기본 구조 :  x_t > cell(have loopd) > y_t
  위 기본 구조에서, x_t는 현재의 입력, y_t는 과거와 현재의 정보를 반영한 출력을 한다. 

: RNN은 입력의 길이만큼 신경망이 펼쳐진다(Unrolled)
: 이때, 입력받는 각 순간을 시점(time step)이라고 한다.
: x_t는 t시점의 입력을 의미한다. 예를 들어 x_2는 두번째 시점의 입력을 의미한다.

RNN의 대표적 유형
: many-to-one : 문서의 단어들ㅇ르 순차적으로 입력받아 해당 문서의 유형을 판단하는 텍스트 분류에 사용될 수 있다.
: One-to-many
: many-to-many : 이는, 각 단어에 레이블을 달아주는 시퀀스 레이블링 작업에 사용된다. 이의 대표적인 예로 개체명 인식을 들 수 있다.
 
Basic archtecture of RNN
출력층과 은닉층으로 나뉘며, 구조식은 다음과 같다.
      출력층 : y_t=어떤 활성화 함수(W_t * h_t + b)
      은닉층 : h_t = tanh(W_h * h_t-1 + W_x * x_t + b) 즉, Hyperbolic Tangent를 통해 현재 시점의 Hidden state인 h_t 연산을 위해
      직전 시점의 hidden state인 h_t-1를 입력받아 업데이트한다. 과거의 정보를 기억하며 죄종 y_n을 향해 진행되는 것이다.
 
 
 RNN : 연속적인 신경망을 처리하기 위함
 
 ** 편향은 최종 결과값의 크기와 필히 같아야 함
  
  
  워드 임베딩 이론
    : N차원 의 실수값으로 들어가게 됨
    : 각각의 단어의 벡터값도 학습에 의존시킴
    : 고유한 벡터로 멥핑시키는 개념
    : 각 단어를 벡터로 바꾸는 ANN으로 이해할 수 있음
    
    학습 후 워드임베딩을 통한 벡터 값을 비교해볼 때, 유사한 뉘양스의 벡터값으로 묶임
    
 RNN의 문제점 Long term dependency problem
 gru는 가중치와 편향개수가 같이 줄어든 관계로 >> 데이터에 비례한 가중치와 편향 관계가 성립하지 못하는 경우가 있음 
 
 many to many 에서는 bidirectional recuurent NN이 더욱 효과적임
